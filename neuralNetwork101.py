# for numpy arrays
import numpy

# for expit() or sigmoid function
import scipy.special

# a simple neural network class
# is based on 3 nodes in each input, hidden(middle), and output layers
# each neuron is a node and a group of nodes is called a layer
# so there are three layers viz input, hidden, and output consisting of 3 nodes

class neuralNetwork:
    
    # initialisations
    # inputNodes - no of inputNodes
    # learningRate - is used to let the change in weights to happen moderately
    
    def __init__(self, inputNodes, hiddenNodes, outputNodes, learningRate):
        
        # set the number of nodes of the neural network
        self.inputNodes = inputNodes
        self.hiddenNodes = hiddenNodes
        self.outputNodes = outputNodes
        
        # set the learning rate (alpha)
        self.learningRate = learningRate
        
        # the activation function of neuron
        
        # sigmoid function given by 
        # y = 1 / (1 + e**-x)
        # used to simulate the biological neuron which collect all the input
        # signals through the dendrites and output/respond by sending its own
        # signal only after some threshold is reached
        
        self.activationFunction = lambda x:scipy.special.expit(x)
        
        # initialise weight matrices
        # weights are assigned randomly at first and then improved iteratively
        
        # random.normal gives a random number within a normal distribution 
        # with mean = 0.0 and standard deviation = 1 / sqrt(no of links)
        
        # weightInput2Hidden - a matrix, size depends on the number of nodes
        # in each layer and 
        # each member represents some weight that is associated between the two
        # neurons of two layers 
        # the larger the weight the stronger the connection and vice-versa
        
        self.weightInput2Hidden = numpy.random.normal(
            0.0, pow(self.hiddenNodes, -0.5), (self.hiddenNodes, self.inputNodes))
        self.weightHidden2Output = numpy.random.normal(
            0.0, pow(self.outputNodes, -0.5), (self.outputNodes, self.hiddenNodes))        
        pass
    
    # train the neural network
    # inputList - list of input data
    # targetList - list of outputs for the given inputs
    # train() tries to minimize the error between targetList and the output 
    # generated by the network adjusting the weights
 
    
    def train(self, inputList, targetList):
        
        # convert inputList and outpuList into 2d arrays
        inputs = numpy.array(inputList, ndmin=2).T
        targets = numpy.array(targetList, ndmin=2).T
        
        # find hidden layer inputs
        
        # numpy.dot() simply means matrix multiplication between two matrices
        # since each input node is connected with all the hidden(inside) nodes
        # with links having some weights
        # so adding products of inputs and weights of links from each and every 
        # input nodes to a particular hidden(inside/next) node gives the
        # required input signal for the hidden(inside/next) node 
        
        # this can be easily achieved with matrix multiplication
        
        hiddenInputs = numpy.dot(self.weightInput2Hidden, inputs)
        
        # find hidden layer outputs
        # to find the output of a node we apply the sigmoid fuction to its input
        
        hiddenOutputs = self.activationFunction(hiddenInputs)
        
        # find final layer inputs
        finalInputs = numpy.dot(self.weightHidden2Output, hiddenOutputs)
        
        # find final layer outputs
        finalOutputs = self.activationFunction(finalInputs)
        
        # now we need to find the error and adjust the weight
        # between the layers to minimise the error
        # gradient descent
        
        # errors at the final layer of n/w
        # we can easily find the error at the output layer using target values
        
        outputErrors = targets - finalOutputs
        
        # now we back propogate the error adjusting the weights
        
        # to calculate the errors in the hidden layer we divide / propogate 
        # the error at an output node to the previous nodes(hidden) based 
        # on the weights of edge / link between the previous nodes 
        # and the output node
        
        # this simply can be achieved using matrix multiplication of
        # outputErrors and transpose of weights from the previous nodes to the
        # output node
        
        # errors in the hidden layer
        # .T indicates transpose
    
        hiddenErrors = numpy.dot(self.weightHidden2Output.T, outputErrors)
        
        
        # update the weights for links between hidden and output layer
        
        # this equation comes from gradient descent method of minimizing error
        # we need to minimize the square of error between actual and calculated
        # values
        # for this we observe how error in a layer changes with the change in 
        # weights of the previous layer
        # we differentiate error wrt weights and simplify using the sigmoid
        # function...
        # finalOutputs - is the output of sigmoid function/activationFunction
        
        self.weightHidden2Output += self.learningRate * numpy.dot((outputErrors * finalOutputs * (1.0 - finalOutputs)), numpy.transpose(hiddenOutputs))
        
        # update the weights for links between the input and hidden layers
        self.weightInput2Hidden += self.learningRate * numpy.dot((hiddenErrors * hiddenOutputs * (1.0 - hiddenOutputs)), numpy.transpose(inputs))

        pass

    
    # query returns the prediction of neural network when passed an input list 
    
    def query(self, inputList):
        # convert the list into a 2d array
        # .T finds the transpose of the resulting 2d matrix
        # ndmin is the min number of dimensions the matrix should have
        inputs = numpy.array(inputList, ndmin=2).T
        
        # hidden layer inputs
        hiddenInputs = numpy.dot(self.weightInput2Hidden, inputs)
        
        # calculate the signals emerging from hidden layer
        hiddenOutputs = self.activationFunction(hiddenInputs)
        
        # final layer inputs
        finalInputs = numpy.dot(self.weightHidden2Output, hiddenOutputs)
        
        # calcuate the final outputs
        finalOutputs = self.activationFunction(finalInputs)
        
        return finalOutputs

