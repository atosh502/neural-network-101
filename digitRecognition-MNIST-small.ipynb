{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numpy arrays\n",
    "import numpy\n",
    "\n",
    "# for expit() or sigmoid function\n",
    "import scipy.special\n",
    "\n",
    "# for 2d plotting\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# for plotting figures within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# this 3 layer neural network is used to predict a smaller dataset of only\n",
    "# 10 images of digits from 0 to 9 after training it with a dataset of 100 \n",
    "# images different from the testing data\n",
    "\n",
    "# derived directly from https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# SMALL DATA:\n",
    "# 100 training data: https://goo.gl/2Uae7W\n",
    "# 10 testing data: https://goo.gl/mtkyy8\n",
    "\n",
    "# LARGE DATA:\n",
    "# 60,000 training data: https://goo.gl/k8wLe2\n",
    "# 10,000 testing data: https://goo.gl/hTvY8p\n",
    "\n",
    "# IN this example only the smaller dataset is used;\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# a simple neural network class\n",
    "# is based on 3 nodes in each input, hidden(middle), and output layers\n",
    "# each neuron is a node and a group of nodes is called a layer\n",
    "# so there are three layers viz input, hidden, and output consisting of 3 nodes\n",
    "\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialisations\n",
    "    # inputNodes - no of inputNodes\n",
    "    # learningRate - is used to let the change in weights to happen moderately\n",
    "    \n",
    "    def __init__(self, inputNodes, hiddenNodes, outputNodes, learningRate):\n",
    "        \n",
    "        # set the number of nodes of the neural network\n",
    "        self.inputNodes = inputNodes\n",
    "        self.hiddenNodes = hiddenNodes\n",
    "        self.outputNodes = outputNodes\n",
    "        \n",
    "        # set the learning rate (alpha)\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        # the activation function of neuron\n",
    "        \n",
    "        # sigmoid function given by \n",
    "        # y = 1 / (1 + e**-x)\n",
    "        # used to simulate the biological neuron which collect all the input\n",
    "        # signals through the dendrites and output/respond by sending its own\n",
    "        # signal only after some threshold is reached\n",
    "        \n",
    "        self.activationFunction = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        # initialise weight matrices\n",
    "        # weights are assigned randomly at first and then improved iteratively\n",
    "        \n",
    "        # random.normal gives a random number within a normal distribution \n",
    "        # with mean = 0.0 and standard deviation = 1 / sqrt(no of links)\n",
    "        \n",
    "        # weightInput2Hidden - a matrix, size depends on the number of nodes\n",
    "        # in each layer and \n",
    "        # each member represents some weight that is associated between the two\n",
    "        # neurons of two layers \n",
    "        # the larger the weight the stronger the connection and vice-versa\n",
    "        \n",
    "        \n",
    "        # the code from book and the github link https://goo.gl/LjG2fa\n",
    "        # mismatch in the second parameter of function numpy.random.normal()\n",
    "        \n",
    "        # in the book self.hiddenNodes is used whereas in the github self.inputNodes is used\n",
    "        # in pow() function\n",
    "        \n",
    "        self.weightInput2Hidden = numpy.random.normal(\n",
    "            0.0, pow(self.inputNodes, -0.5), (self.hiddenNodes, self.inputNodes))\n",
    "        \n",
    "        # in the book self.outputNodes is used whereas in the github self.hiddenNodes is used\n",
    "        # in pow() function\n",
    "        self.weightHidden2Output = numpy.random.normal(\n",
    "            0.0, pow(self.hiddenNodes, -0.5), (self.outputNodes, self.hiddenNodes))        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    # inputList - list of input data\n",
    "    # targetList - list of outputs for the given inputs\n",
    "    # train() tries to minimize the error between targetList and the output \n",
    "    # generated by the network adjusting the weights\n",
    " \n",
    "    \n",
    "    def train(self, inputList, targetList):\n",
    "        \n",
    "        # convert inputList and outpuList into 2d arrays\n",
    "        inputs = numpy.array(inputList, ndmin=2).T\n",
    "        targets = numpy.array(targetList, ndmin=2).T\n",
    "        \n",
    "        # find hidden layer inputs\n",
    "        \n",
    "        # numpy.dot() simply means matrix multiplication between two matrices\n",
    "        # since each input node is connected with all the hidden(inside) nodes\n",
    "        # with links having some weights\n",
    "        # so adding products of inputs and weights of links from each and every \n",
    "        # input nodes to a particular hidden(inside/next) node gives the\n",
    "        # required input signal for the hidden(inside/next) node \n",
    "        \n",
    "        # this can be easily achieved with matrix multiplication\n",
    "        \n",
    "        hiddenInputs = numpy.dot(self.weightInput2Hidden, inputs)\n",
    "        \n",
    "        # find hidden layer outputs\n",
    "        # to find the output of a node we apply the sigmoid fuction to its input\n",
    "        \n",
    "        hiddenOutputs = self.activationFunction(hiddenInputs)\n",
    "        \n",
    "        # find final layer inputs\n",
    "        finalInputs = numpy.dot(self.weightHidden2Output, hiddenOutputs)\n",
    "        \n",
    "        # find final layer outputs\n",
    "        finalOutputs = self.activationFunction(finalInputs)\n",
    "        \n",
    "        # now we need to find the error and adjust the weight\n",
    "        # between the layers to minimise the error\n",
    "        # gradient descent\n",
    "        \n",
    "        # errors at the final layer of n/w\n",
    "        # we can easily find the error at the output layer using target values\n",
    "        \n",
    "        outputErrors = targets - finalOutputs\n",
    "        \n",
    "        # now we back propogate the error adjusting the weights\n",
    "        \n",
    "        # to calculate the errors in the hidden layer we divide / propogate \n",
    "        # the error at an output node to the previous nodes(hidden) based \n",
    "        # on the weights of edge / link between the previous nodes \n",
    "        # and the output node\n",
    "        \n",
    "        # this simply can be achieved using matrix multiplication of\n",
    "        # outputErrors and transpose of weights from the previous nodes to the\n",
    "        # output node\n",
    "        \n",
    "        # errors in the hidden layer\n",
    "        # .T indicates transpose\n",
    "    \n",
    "        hiddenErrors = numpy.dot(self.weightHidden2Output.T, outputErrors)\n",
    "        \n",
    "        \n",
    "        # update the weights for links between hidden and output layer\n",
    "        \n",
    "        # this equation comes from gradient descent method of minimizing error\n",
    "        # we need to minimize the square of error between actual and calculated\n",
    "        # values\n",
    "        # for this we observe how error in a layer changes with the change in \n",
    "        # weights of the previous layer\n",
    "        # we differentiate error wrt weights and simplify using the sigmoid\n",
    "        # function...\n",
    "        # finalOutputs - is the output of sigmoid function/activationFunction\n",
    "        \n",
    "        self.weightHidden2Output += self.learningRate * numpy.dot((outputErrors * finalOutputs * (1.0 - finalOutputs)), numpy.transpose(hiddenOutputs))\n",
    "        \n",
    "        # update the weights for links between the input and hidden layers\n",
    "        self.weightInput2Hidden += self.learningRate * numpy.dot((hiddenErrors * hiddenOutputs * (1.0 - hiddenOutputs)), numpy.transpose(inputs))\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query returns the prediction of neural network when passed an input list \n",
    "    \n",
    "    def query(self, inputList):\n",
    "        # convert the list into a 2d array\n",
    "        # .T finds the transpose of the resulting 2d matrix\n",
    "        # ndmin is the min number of dimensions the matrix should have\n",
    "        inputs = numpy.array(inputList, ndmin=2).T\n",
    "        \n",
    "        # hidden layer inputs\n",
    "        hiddenInputs = numpy.dot(self.weightInput2Hidden, inputs)\n",
    "        \n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hiddenOutputs = self.activationFunction(hiddenInputs)\n",
    "        \n",
    "        # final layer inputs\n",
    "        finalInputs = numpy.dot(self.weightHidden2Output, hiddenOutputs)\n",
    "        \n",
    "        # calcuate the final outputs\n",
    "        finalOutputs = self.activationFunction(finalInputs)\n",
    "        \n",
    "        return finalOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets talk about the data\n",
    "# each image data contains a single line representing the label--integer or the actual number\n",
    "# in the image and the rest 784 integers contains the pixel positions for the actual digit\n",
    "# in an image of size 28 by 28\n",
    "\n",
    "# so we see that at the first layer we need to feed 784 total inputs to the neural network\n",
    "# secondly the number 100 is choosen so that it is neither greater than 784 and nor lesser\n",
    "# than a number which isn't able to find enough links between the neurons\n",
    "# lastly the number 10 is choosen as the number of nodes for the final layer because we need\n",
    "# to have 10 different outputs for ten different numbers from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of nodes\n",
    "inputNodes = 784\n",
    "outputNodes = 10\n",
    "# hiddenNodes = 100\n",
    "# a better tweak\n",
    "hiddenNodes = 200\n",
    "\n",
    "# set the learning rate\n",
    "# learningRate = 0.3\n",
    "# best learning rate for MNIST dataset\n",
    "learningRate = 0.1\n",
    "\n",
    "# finally let's create an instance of a neural network\n",
    "n = neuralNetwork(inputNodes, hiddenNodes, outputNodes, learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will load analyse the data\n",
    "\n",
    "# loading the MNIST training data CSV file\n",
    "# ---------------------------------------------------------\n",
    "# REMEMBER to download the file before running\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "trainingDataFile = open('mnist_dataset/mnist_train_100.csv', 'r')\n",
    "trainingDataList = trainingDataFile.readlines()\n",
    "trainingDataFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual number=  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc61a413cf8>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4QeCIvxAUP8PRZ8Vlgh2BcUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc612811ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking the data\n",
    "image = trainingDataList[0].split(',')\n",
    "imageValues = numpy.asfarray(image[1:]).reshape(28, 28)\n",
    "print('actual number= ', image[0])\n",
    "matplotlib.pyplot.imshow(imageValues, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to scale the input data so that it is between 0.01 and 0.99\n",
    "# i.e. within the range of output of the activation function\n",
    "# so we scale the input data between 0.01 and 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's train the neural network\n",
    "epochs = 7\n",
    "for e in range(epochs):\n",
    "    for record in trainingDataList:\n",
    "\n",
    "        image = record.split(',')\n",
    "\n",
    "        # scale the individual pixels between 0.01 to 1.0\n",
    "        # asfarray() converts string into float\n",
    "        imageValues = (numpy.asfarray(image[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "        # create the target array to store the output values\n",
    "        targets = numpy.zeros(outputNodes) + 0.01\n",
    "        targets[int(image[0])] = 0.99\n",
    "\n",
    "        # train the neural network\n",
    "        n.train(imageValues, targets)\n",
    "        pass\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the validation file\n",
    "# ---------------------------------------------------------\n",
    "# REMEMBER to download the file before running\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "testingDataFile = open('mnist_dataset/mnist_test_10.csv', 'r')\n",
    "testingDataList = testingDataFile.readlines()\n",
    "testingDataFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual number= 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc61a408940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing data\n",
    "image = testingDataList[0].split(',')\n",
    "imageValues = numpy.asfarray(image[1:]).reshape(28, 28)\n",
    "matplotlib.pyplot.imshow(imageValues, cmap='Greys', interpolation='None')\n",
    "print('actual number=', image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08198146],\n",
       "       [ 0.01824928],\n",
       "       [ 0.05526444],\n",
       "       [ 0.05751246],\n",
       "       [ 0.05883327],\n",
       "       [ 0.03480678],\n",
       "       [ 0.00798773],\n",
       "       [ 0.8017138 ],\n",
       "       [ 0.06747322],\n",
       "       [ 0.06020293]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the neural network returns 7 for the input data\n",
    "n.query(((numpy.asfarray(image[1:]) / 255.0) * 0.99) + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorecard for how well the network performs\n",
    "scorecard = []\n",
    "\n",
    "for record in testingDataList:\n",
    "    image = record.split(',')\n",
    "    correctLabel = int(image[0])\n",
    "    \n",
    "    print(correctLabel, \" correct number\")\n",
    "    \n",
    "    # scale the inputs\n",
    "    inputs = ((numpy.asfarray(image[1:]) / 255.0) * 0.99) + 0.01\n",
    "    \n",
    "    # take prediction from the network\n",
    "    outputs = n.query(inputs)\n",
    "    \n",
    "    # take the index of the highest value corresponding to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    print(label, \" network's answer\")\n",
    "    \n",
    "    if (label == correctLabel):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)   \n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.6\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score\n",
    "score = numpy.asarray(scorecard)\n",
    "print('performance = ', score.sum() / score.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
